{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import lidar_data_processing\n",
    "\n",
    "import pandas as pd\n",
    "from laspy.file import File\n",
    "from pickle import dump, load\n",
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as udata\n",
    "from torch.autograd import Variable\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Data parameters\n",
    "scan_line_gap_break = 7000 # threshold over which scan_gap indicates a new scan line\n",
    "min_pt_count = 1730 # in a scan line, otherwise line not used\n",
    "max_pt_count = 2000 # in a scan line, otherwise line not used\n",
    "num_scan_lines = 2000 # to use as training set\n",
    "starting_line=1000\n",
    "val_split = 0.2\n",
    "seq_len = 64\n",
    "\n",
    "# Angle range of considered points (deg /0.006)\n",
    "starting_angle = 4500\n",
    "ending_angle = -4500\n",
    "\n",
    "# points in between scan lines\n",
    "stride_inline = 5\n",
    "stride_across_lines = 3\n",
    "\n",
    "# Note: x_scaled, y_scaled, and z_scaled MUST be the first 3 features and miss_pts_before MUST be the last feature\n",
    "feature_list = [\n",
    "    'x_scaled',\n",
    "    'y_scaled',\n",
    "    'z_scaled',\n",
    "    'scan_line_idx',\n",
    "    'scan_angle_deg',\n",
    "    'abs_scan_angle_deg',\n",
    "    'miss_pts_before'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/michael/anaconda3/lib/python3.8/site-packages/pandas/core/computation/expressions.py:203: UserWarning: evaluating in Python space because the '*' operator is not supported by numexpr for the bool dtype, use '&' instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not enough points in line 3983\n"
     ]
    }
   ],
   "source": [
    "# scan_line_tensor is of dimension [num_scan_lines,pts_per_line,num_features]\n",
    "scan_line_tensor = lidar_data_processing.create_scan_line_tensor()\n",
    "scan_line_tensor = scan_line_tensor[:-1,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scan_line_tensor.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: the stride_inline and stride_across_lines only impact where sample squares are placed. The squares themselves do not skip lines in either dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sample_idx_2d(data,\n",
    "                       starting_line, \n",
    "                       num_scan_lines, \n",
    "                       seq_len,\n",
    "                       stride_inline,\n",
    "                       stride_across_lines,\n",
    "                       sc):\n",
    "    '''\n",
    "    Function generates training and validation samples for filling\n",
    "    randomly chosen missing points.\n",
    "    Inputs:\n",
    "        data: 3-Tensor with dimensions: i) the number of viable scan lines in the flight pass, \n",
    "                                        ii) the minimum number of points in the scan line,\n",
    "                                        iii) 3 (xyz, or feature count)\n",
    "    \n",
    "    '''\n",
    "    # Create generic x tensor - now accounts for stride\n",
    "    # Number of samples per scan line, accounting for stride\n",
    "    seq_per_line = int((data.shape[1]-seq_len)/stride_inline+1)\n",
    "    # Number of samples across scan lines, accounting for stride\n",
    "    seq_across_lines = int((num_scan_lines - seq_len)/stride_across_lines+1)\n",
    "\n",
    "    valid_idx_list = []\n",
    "    # Cycle through the number of scan lines requested\n",
    "    # This now strides based on stride_across_lines\n",
    "    for i in range(starting_line,(starting_line+ \\\n",
    "                                      stride_across_lines*seq_across_lines),stride_across_lines):\n",
    "        # For each viable starting line, loop through possible patches (accounting for stride)\n",
    "        for j in range(0,seq_per_line*stride_inline,stride_inline):\n",
    "            # does the patch with top left at data[i,j] have missing points?\n",
    "            if data[i:i+seq_len,j:j+seq_len,-1].min()==0.:\n",
    "                # Add index to the list\n",
    "                valid_idx_list.append([i,j])\n",
    "    return valid_idx_list\n",
    "\n",
    "\n",
    "def sliding_windows2d(data, seq_len, seq_per_line, line_num, stride_inline, sc, x):\n",
    "    '''Given the scan_line_tensor as data and a line number, function iterates over the line, creating the \n",
    "        specified sequences (each is a sample).\n",
    "        This also removes the miss_pts_before column to conserve memory'''\n",
    "    for i,start_idx in enumerate(range(0,seq_per_line*stride_inline,stride_inline)):\n",
    "        # sample_idx considers previous lines\n",
    "        sample_idx = i+line_num*seq_per_line\n",
    "        _x = data[:seq_len,i:i+seq_len,:]\n",
    "        if _x[:,:,-1].min() == 0.:\n",
    "            x.append(_x[:,:,:-1]) # Remove miss_pts_before\n",
    "    return x\n",
    "\n",
    "def min_max_tensor(tensor):\n",
    "    ''' Function takes a 4-D tensor, performs minmax scaling to [0,1] along the third dimension.\n",
    "    MinMaxScaler will be created.  '''\n",
    "    print(\"tensor shape: \",tensor.shape)\n",
    "    # Remove infilled points\n",
    "    condition = tensor[:,:,-1] !=-1.\n",
    "    t = tensor[condition]\n",
    "    print(\"min miss_pts: \",t[:,-1].min())\n",
    "    t = t.contiguous()\n",
    "    sc =  MinMaxScaler()\n",
    "    sc.fit(t[:,:-1])\n",
    "    \n",
    "    return sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor shape:  torch.Size([1600, 1730, 7])\n",
      "min miss_pts:  tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "# Split the scan lines into train and test, then generate x_train and x_val\n",
    "num_scan_lines_train = int(num_scan_lines*(1-val_split))\n",
    "num_scan_lines_val = num_scan_lines - num_scan_lines_train\n",
    "\n",
    "sc = min_max_tensor(scan_line_tensor[starting_line:starting_line+num_scan_lines_train,:,:])\n",
    "\n",
    "train_idx_list = generate_sample_idx_2d(scan_line_tensor,starting_line,\n",
    "                                        num_scan_lines_train,seq_len,stride_inline,\n",
    "                                        stride_across_lines,sc)\n",
    "\n",
    "valid_idx_list = generate_sample_idx_2d(scan_line_tensor,starting_line+num_scan_lines_train,\n",
    "                                        num_scan_lines_val,seq_len,stride_inline,\n",
    "                                        stride_across_lines,sc)\n",
    "\n",
    "# sc.transform of scan_line_tensor, after identifying valid indices\n",
    "# Remove miss_pts_before\n",
    "scan_line_tensor = scan_line_tensor[:,:,:-1]\n",
    "slt_out = torch.Tensor(sc.transform(scan_line_tensor.reshape(-1,scan_line_tensor.shape[-1])).reshape(scan_line_tensor.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-890.1019)\n",
      "tensor(-26.8140)\n"
     ]
    }
   ],
   "source": [
    "print(slt_out.min())\n",
    "print(scan_line_tensor.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save files \n",
    "torch.save(slt_out,'../lidar_data/'+str(seq_len)+'_'+str(seq_len)+'/'+'scan_line_tensor.pts')\n",
    "torch.save(train_idx_list,'../lidar_data/'+str(seq_len)+'_'+str(seq_len)+'/'+'train_idx_list.pts')\n",
    "torch.save(valid_idx_list,'../lidar_data/'+str(seq_len)+'_'+str(seq_len)+'/'+'valid_idx_list.pts')\n",
    "torch.save(sc,'../lidar_data/'+str(seq_len)+'_'+str(seq_len)+'/'+'sc.pts')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_missing_pts(first_return_df):\n",
    "    # Create a series with the indices of points after gaps and the number of missing points (no max)\n",
    "    miss_pt_ser = first_return_df[first_return_df['miss_pts_before']>0]['miss_pts_before']\n",
    "    # miss_pts_arr is an array of zeros that is the dimensions [num_missing_pts,cols_in_df]\n",
    "    miss_pts_arr = np.zeros([int(miss_pt_ser.sum()),first_return_df.shape[1]])\n",
    "    # Create empty series to collect the indices of the missing points\n",
    "    indices = np.ones(int(miss_pt_ser.sum()))\n",
    "\n",
    "    # Fill in the indices, such that they all slot in in order before the index\n",
    "    i=0\n",
    "    for index, row in zip(miss_pt_ser.index,miss_pt_ser):\n",
    "        new_indices = index + np.arange(row)/row-1+.01\n",
    "        indices[i:i+int(row)] = new_indices\n",
    "        i+=int(row)\n",
    "    # Create a Dataframe of the indices and miss_pts_arr\n",
    "    miss_pts_df = pd.DataFrame(miss_pts_arr,index=indices,columns = first_return_df.columns)\n",
    "    miss_pts_df['mask'] = [0]*miss_pts_df.shape[0]\n",
    "    miss_pts_df['miss_pts_before'] = -1\n",
    "    # Fill scan fields with NaN so we can interpolate them\n",
    "    for col in ['scan_angle','scan_angle_deg']:\n",
    "        miss_pts_df[col] = [np.NaN]*miss_pts_df.shape[0]\n",
    "    # Concatenate first_return_df and new df\n",
    "    full_df = first_return_df.append(miss_pts_df, ignore_index=False)\n",
    "    # Resort so that the missing points are interspersed, and then reset the index\n",
    "    full_df = full_df.sort_index().reset_index(drop=True)\n",
    "    # Interpolate the scan angles\n",
    "    full_df[['scan_angle','scan_angle_deg']] = full_df[['scan_angle','scan_angle_deg']].interpolate()\n",
    "    # Fill miss_pts_before with -1 so infilled points can be identified\n",
    "    \n",
    "    return full_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_return_df = pd.read_pickle(\"../../lidar/Data/parking_lot/first_returns_modified_164239.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# miss_pts_before is the count of missing points before the point in question (scan gap / 5 -1)\n",
    "first_return_df['miss_pts_before'] = round((first_return_df['scan_gap']/-5)-1)\n",
    "first_return_df['miss_pts_before'] = [max(0,pt) for pt in first_return_df['miss_pts_before']]\n",
    "\n",
    "# Add 'mask' column, set to one by default\n",
    "first_return_df['mask'] = [1]*first_return_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add abs_scan_angle_deg as a feature\n",
    "first_return_df['abs_scan_angle_deg'] = abs(first_return_df['scan_angle_deg'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract tensor of scan lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of points per scan line\n",
    "scan_line_pt_count = first_return_df.groupby('scan_line_idx').count()['gps_time']\n",
    "\n",
    "# Remove scan lines outside the point count range from first_return_df\n",
    "valid_scan_line_idx = scan_line_pt_count[(scan_line_pt_count>min_pt_count) * (scan_line_pt_count<max_pt_count)].index\n",
    "\n",
    "# Only the points that are in valid scan lines\n",
    "first_return_df_valid = first_return_df[first_return_df['scan_line_idx'].isin(valid_scan_line_idx)]\n",
    "\n",
    "# # Minimum and maximum scan_angle_deg per scan line\n",
    "# min_scan_angle_deg = first_return_df.groupby('scan_line_idx').min()['scan_angle_deg']\n",
    "# max_scan_angle_deg = first_return_df.groupby('scan_line_idx').max()['scan_angle_deg']\n",
    "\n",
    "# Identify the indices for points at end of scan lines\n",
    "# scan_break_idx = first_return_df[(first_return_df['scan_gap']>scan_line_gap_break)].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in missing points``\n",
    "first_return_df_valid = add_missing_pts(first_return_df_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_return_df_valid.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now remove lines that don't have 1730 points between -27 and 27 degrees\n",
    "# Number of points per scan line\n",
    "scan_line_pt_count = first_return_df_valid.groupby('scan_line_idx').count()['gps_time']\n",
    "\n",
    "# Remove scan lines outside the point count range from first_return_df\n",
    "valid_scan_line_idx = scan_line_pt_count[scan_line_pt_count>min_pt_count].index\n",
    "\n",
    "# Only the points that are in valid scan lines\n",
    "first_return_df_valid = first_return_df_valid[first_return_df_valid['scan_line_idx'].isin(valid_scan_line_idx)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indices for the point closes to starting_angle in each scan line\n",
    "starting_idx = [abs(first_return_df_valid[first_return_df_valid['scan_line_idx']==line_idx] \\\n",
    "     ['scan_angle']-starting_angle).argmin() for line_idx in first_return_df_valid['scan_line_idx'].unique()]\n",
    "\n",
    "# Remove the nan idx corresponding to zero scan line\n",
    "starting_idx = [x for x in starting_idx if str(x) != 'nan']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Tensor\n",
    "scan_line_tensor = torch.randn([len(starting_idx),min_pt_count,len(feature_list)])\n",
    "\n",
    "# Loop thru scan lines\n",
    "for line,line_idx in enumerate(starting_idx):\n",
    "        # Fill the appropriate line in scan_line_tensor\n",
    "        name = first_return_df_valid.iloc[line_idx].name\n",
    "        try:\n",
    "            scan_line_tensor[line,:,:] = torch.Tensor(first_return_df_valid.loc\\\n",
    "                                      [name:name+min_pt_count-1][feature_list].values)\n",
    "        except RuntimeError:\n",
    "            print(\"Not enough points in line {}\".format(line))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del([first_return_df,first_return_df_valid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function inputs\n",
    "samples_per_file = 1000\n",
    "# Split the scan lines into train and test, then generate x_train and x_val\n",
    "num_scan_lines = 100\n",
    "num_scan_lines_train = int(num_scan_lines*(1-val_split))\n",
    "num_scan_lines_val = num_scan_lines - num_scan_lines_train\n",
    "\n",
    "\n",
    "# Function is called on just the training scan_lines\n",
    "data = scan_line_tensor[starting_line:starting_line+num_scan_lines_train]\n",
    "\n",
    "# IN THE FUNCTION\n",
    "# Number of samples per scan line, accounting for stride\n",
    "seq_per_line = int((data.shape[1]-seq_len)/stride_inline+1)\n",
    "\n",
    "# Number of samples across scan lines, accounting for stride\n",
    "seq_across_lines = int((num_scan_lines - seq_len)/stride_across_lines+1)\n",
    "\n",
    "sample_count = seq_per_line*seq_across_lines\n",
    "\n",
    "print(sample_count/samples_per_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_trans = sc.transform(x_train_list[0].reshape(-1,x_train_list[0].shape[2])).reshape(x_train_list[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del scan_line_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val_list,sc = torch.load('x_val_raw.pt')\n",
    "# torch.save(x_val_list_norm,'x_val.pt')\n",
    "# torch.save([x_val_list,sc],'x_val_raw.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val_list_norm = [sc.transform(x.reshape(-1,x.shape[2])).reshape(x.shape) for x in x_val_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(x_val_list_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blocks = 10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Old version of functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original version\n",
    "def generate_samples2d(data,seq_len,\n",
    "                       stride_inline,\n",
    "                       stride_across_lines,\n",
    "                       sc, \n",
    "                       samples_per_file = 2000,\n",
    "                       file_dir = '../lidar_data/train/', filename = 'x_train_'):\n",
    "    '''\n",
    "    Function generates training and validation samples for filling\n",
    "    randomly chosen missing points.\n",
    "    Inputs:\n",
    "        data: 3-Tensor with dimensions: i) the number of viable scan lines in the flight pass, \n",
    "                                        ii) the minimum number of points in the scan line,\n",
    "                                        iii) 3 (xyz, or feature count)\n",
    "    \n",
    "    '''\n",
    "    # Create generic x tensor - now accounts for stride\n",
    "    # Number of samples per scan line, accounting for stride\n",
    "    seq_per_line = int((data.shape[1]-seq_len)/stride_inline+1)\n",
    "    # Number of samples across scan lines, accounting for stride\n",
    "    seq_across_lines = int((data.shape[0] - seq_len)/stride_across_lines+1)\n",
    "    \n",
    "    x_list = []\n",
    "    file_count = 0\n",
    "    # Cycle through the number of scan lines requested\n",
    "    # This now strides over some lines based on stride_across_lines\n",
    "    for j,line_idx in enumerate(range(0,(stride_across_lines*seq_across_lines),stride_across_lines)):\n",
    "        # line_idx is the scan_line_idx\n",
    "        x_list = sliding_windows2d(data[line_idx:line_idx+seq_len,:,:], \\\n",
    "                                   seq_len,seq_per_line,j, stride_inline, sc, x_list)\n",
    "#         # write file when x_list is long\n",
    "#         if len(x_list) > samples_per_file:\n",
    "#             x_out = [sc.transform(x.reshape(-1,x.shape[2])).reshape(x.shape) for x in x_list]\n",
    "#             torch.save(x_out,file_dir+filename+str(file_count)+'.pts')\n",
    "#             file_count+=1\n",
    "#             x_list = []\n",
    "#             print(\"Wrote file: {}\".format(file_dir+filename+str(file_count)))\n",
    "        # Write each element in x_list to its own file\n",
    "        x_out = [sc.transform(x.reshape(-1,x.shape[2])).reshape(x.shape) for x in x_list]\n",
    "        print(\"x_out length: \",len(x_out))\n",
    "        while len(x_out)>0:\n",
    "            file = x_out.pop()\n",
    "            torch.save(file,file_dir+filename+str(file_count)+'.pts')\n",
    "            file_count+=1\n",
    "        print(\"Wrote files for line {}\".format(j))\n",
    "    \n",
    "def make_one_tensor(x_list,train=True,sc=None):    \n",
    "    x = tensor_list_combine(x_list)  \n",
    "\n",
    "    # Remove the 'miss_pts_before' column\n",
    "    x = x[:,:,:,:-1]\n",
    "\n",
    "    # Different for train and val\n",
    "    # Standardize the data \n",
    "    if train:\n",
    "        x_norm_dim, sc = min_max_tensor(x)\n",
    "    else:\n",
    "        x_norm_dim, sc = min_max_tensor(x,sc)\n",
    "    \n",
    "    # Reorder to [row_count,feat_count,seq_len]\n",
    "    x_norm = x_norm_dim.permute([0,3,1,2])\n",
    "\n",
    "    return x_norm, sc\n",
    "\n",
    "def sliding_windows2d(data, seq_len, seq_per_line, line_num, stride_inline, sc, x):\n",
    "    '''Given the scan_line_tensor as data and a line number, function iterates over the line, creating the \n",
    "        specified sequences (each is a sample).\n",
    "        This also removes the miss_pts_before column to conserve memory'''\n",
    "    for i,start_idx in enumerate(range(0,seq_per_line*stride_inline,stride_inline)):\n",
    "        # sample_idx considers previous lines\n",
    "        sample_idx = i+line_num*seq_per_line\n",
    "        _x = data[:seq_len,i:i+seq_len,:]\n",
    "        if _x[:,:,-1].min() == 0.:\n",
    "            x.append(_x[:,:,:-1]) # Remove miss_pts_before\n",
    "    return x\n",
    "\n",
    "def tensor_list_combine(tens_list):\n",
    "    ''' Given a list of 3-D tensors with equal dimensions, function concatenates them into a 4D tensor'''\n",
    "    new_tens = torch.Tensor(len(tens_list),tens_list[0].shape[0],tens_list[0].shape[1],tens_list[0].shape[2])\n",
    "    for i,tensor in enumerate(tens_list):\n",
    "        new_tens[i,:,:,:] = tensor\n",
    "    return new_tens\n",
    "\n",
    "def min_max_tensor(tensor):\n",
    "    ''' Function takes a 4-D tensor, performs minmax scaling to [0,1] along the third dimension.\n",
    "    If in train mode, MinMaxScaler will be created.  If train=False, the scaler provided will be used.'''\n",
    "\n",
    "    # Remove infilled points\n",
    "    condition = tensor[:,:,-1] !=-1.\n",
    "    tensor = tensor[condition]\n",
    "    \n",
    "    # Train MinMaxScaler, return the scaler\n",
    "    tensor = tensor.contiguous()\n",
    "    sc =  MinMaxScaler()\n",
    "    sc.fit(tensor)\n",
    "    \n",
    "    return sc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
