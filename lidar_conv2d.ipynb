{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from utils import data\n",
    "import models, utils\n",
    "\n",
    "import pandas as pd\n",
    "from laspy.file import File\n",
    "from pickle import dump, load\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as udata\n",
    "from torch.autograd import Variable\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import lidar_data_processing\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args(object):\n",
    "    def __init__(self):\n",
    "        self.data_path= 'data' # not used\n",
    "        self.dataset= 'masked_pwc' # move lidar into datasets\n",
    "        self.batch_size= 64\n",
    "        self.model= 'lidar_unet2d_old'\n",
    "        self.in_channels = 6\n",
    "        self.out_channels = 3\n",
    "        self.lr= 0.005\n",
    "        self.weight_decay = 0.\n",
    "        self.num_epochs= 50\n",
    "        self.min_sep = 5 # not used\n",
    "        self.num_scan_lines = 1000\n",
    "        self.seq_len = 32\n",
    "        self.scan_line_gap_break = 7000 # threshold over which scan_gap indicates a new scan line\n",
    "        self.min_pt_count = 1700 # in a scan line, otherwise line not used\n",
    "        self.max_pt_count = 2000 # in a scan line, otherwise line not used\n",
    "        self.mask_pts_per_seq = 5\n",
    "        self.mask_consecutive = True\n",
    "        # points in between scan lines\n",
    "        self.stride_inline = 5\n",
    "        self.stride_across_lines = 3\n",
    "        self.valid_interval= 1 \n",
    "        self.save_interval= 1\n",
    "        self.seed = 0\n",
    "#         self.experiment_dir = 'lidar_experiments/2d'\n",
    "        self.output_dir= '../lidar_experiments/2d'\n",
    "#         self.checkpoint_dir= 'lidar_experiments/2d'\n",
    "        self.MODEL_PATH_LOAD = \"../lidar_experiments/2d/lidar_unet2d/lidar-unet2d-Nov-09-21_16_11/checkpoints/checkpoint_best.pt\"\n",
    "        self.experiment= ''\n",
    "        self.resume_training= False\n",
    "        self.restore_file= None\n",
    "        self.no_save= False\n",
    "        self.step_checkpoints= False\n",
    "        self.no_log= False\n",
    "        self.log_interval= 100\n",
    "        self.no_visual= False\n",
    "        self.visual_interval= 100\n",
    "        self.no_progress= False\n",
    "        self.draft= False\n",
    "        self.dry_run= False\n",
    "        self.bias= False \n",
    "#         self.in_channels= 1 # maybe 6?\n",
    "        self.test_num = 0\n",
    "        # UNET\n",
    "        self.residual = False\n",
    "        self.wtd_loss = True\n",
    "        self.batch_norm = True\n",
    "args=Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpu or cpu\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "args = utils.setup_experiment(args)\n",
    "utils.init_logging(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Saving model\n",
    "# MODEL_PATH = \"models/lidar/conv1d_256seq_400epochs_092620.pth\"\n",
    "# torch.save(model.state_dict(), MODEL_PATH)\n",
    "\n",
    "# Loading models\n",
    "\n",
    "train_new_model = False\n",
    "\n",
    "# Build data loaders, a model and an optimizer\n",
    "if train_new_model:\n",
    "    model = models.build_model(args).to(device)\n",
    "else:\n",
    "    model = models.build_model(args)\n",
    "    model.load_state_dict(torch.load(args.MODEL_PATH_LOAD)['model'][0])\n",
    "    model.to(device)\n",
    "\n",
    "print(model)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args.lr, weight_decay = args.weight_decay)\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer,milestones=[5,15,30,50,500], gamma=0.5)\n",
    "logging.info(f\"Built a model consisting of {sum(p.numel() for p in model.parameters()):,} parameters\")\n",
    "\n",
    "if args.resume_training:\n",
    "    state_dict = utils.load_checkpoint(args, model, optimizer, scheduler)\n",
    "    global_step = state_dict['last_step']\n",
    "    start_epoch = int(state_dict['last_step']/(403200/state_dict['args'].batch_size))+1\n",
    "else:\n",
    "    global_step = -1\n",
    "    start_epoch = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the pts files\n",
    "`scan_line_tensor` is the data file, a 3-D tensor of size [num_scan_lines,pts_per_scan_line,num_feats].  \n",
    "`idx_lists` indicate the top left corner of each training or validation square.  \n",
    "`sc` is the minmaxscaler used to generate the training set. Needed here to calculate weightedMSELoss at real scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/michael/.local/lib/python3.8/site-packages/sklearn/base.py:310: UserWarning: Trying to unpickle estimator MinMaxScaler from version 0.22.1 when using version 0.24.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Loads as a list of numpy arrays\n",
    "scan_line_tensor = torch.load('../lidar_data/32_32/'+'scan_line_tensor.pts')\n",
    "train_idx_list = torch.load('../lidar_data/32_32/'+'train_idx_list.pts')\n",
    "valid_idx_list = torch.load('../lidar_data/32_32/'+'valid_idx_list.pts')\n",
    "sc = torch.load('../lidar_data/32_32/'+'sc.pts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_mask(sample,mask_pts_per_seq,consecutive=True):\n",
    "    # Given a 3-D tensor of all ones, returns a mask_tensor of same shape \n",
    "    # with random masking determined by mask_pts_per_seq\n",
    "    mask_tensor = torch.ones(sample.shape)\n",
    "    seq_len = mask_tensor.shape[0]\n",
    "    \n",
    "    if consecutive:\n",
    "        # Creates a square of missing points\n",
    "        first_mask = int(np.random.choice(np.arange(8,seq_len-8-mask_pts_per_seq),1))\n",
    "        \n",
    "        mask_tensor[first_mask:first_mask+mask_pts_per_seq,first_mask:first_mask+mask_pts_per_seq,:] = 0\n",
    "            \n",
    "    else:\n",
    "        # TO DO: Random points throughout the patch\n",
    "        for i in range(sample.shape[0]):\n",
    "            m[i,:] = np.random.choice(np.arange(8,seq_len-8),mask_pts_per_seq,replace=False)\n",
    "\n",
    "    return mask_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloader class\n",
    "class LidarLstmDataset(udata.Dataset):\n",
    "    def __init__(self, scan_line_tensor, idx_list, seq_len = 64, mask_pts_per_seq = 5, consecutive = True):\n",
    "        super(LidarLstmDataset, self).__init__()\n",
    "        self.scan_line_tensor = scan_line_tensor\n",
    "        self.idx_list = idx_list\n",
    "        self.seq_len = seq_len\n",
    "        self.mask_pts_per_seq = mask_pts_per_seq\n",
    "        self.consecutive = consecutive\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx_list)\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        row = self.idx_list[index][0]\n",
    "        col = self.idx_list[index][1]\n",
    "        clean = self.scan_line_tensor[row:row+self.seq_len,col:col+self.seq_len,:]\n",
    "        mask = add_mask(clean,self.mask_pts_per_seq,self.consecutive)\n",
    "        return clean.permute(2,0,1), mask.permute(2,0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'args' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-7c8027f62729>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLidarLstmDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscan_line_tensor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_idx_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmask_pts_per_seq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mvalid_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLidarLstmDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscan_line_tensor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalid_idx_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmask_pts_per_seq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtrain_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mvalid_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'args' is not defined"
     ]
    }
   ],
   "source": [
    "train_dataset = LidarLstmDataset(scan_line_tensor,train_idx_list,args.seq_len, args.mask_pts_per_seq)\n",
    "valid_dataset = LidarLstmDataset(scan_line_tensor,valid_idx_list,args.seq_len, args.mask_pts_per_seq)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, num_workers=4, shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=32, num_workers=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a loss function that weights the loss according to coordinate ranges (xmax-xmin, ymax-ymin, zmax-zmin)\n",
    "def weighted_MSELoss(pred,true,sc,mask_pts_per_seq=args.mask_pts_per_seq):\n",
    "    '''weighted_MSELoss reconverts MSE loss back to the original scale of x,y,z.\n",
    "    Rationale is because xyz have such different ranges, we don't want to ignore the ones with largest scale.\n",
    "    Assumes that x,y,z are the first 3 features in sc scaler'''\n",
    "    \n",
    "    ranges = torch.Tensor(sc.data_max_[:3]-sc.data_min_[:3])\n",
    "    raw_loss = torch.zeros(3,dtype=float)\n",
    "    for i in range(3):\n",
    "        raw_loss[i] = F.mse_loss(pred[:,i,:,:], true[:,i,:,:], reduction=\"sum\") \n",
    "    return (ranges**2 * raw_loss).sum() #/ (pred.shape[0]*mask_pts_per_seq**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Track moving average of loss values\n",
    "train_meters = {name: utils.RunningAverageMeter(0.98) for name in ([\"train_loss\"])}\n",
    "valid_meters = {name: utils.AverageMeter() for name in ([\"valid_loss\"])}\n",
    "writer = SummaryWriter(log_dir=args.experiment_dir) if not args.no_visual else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# TRAINING\n",
    "for epoch in range(start_epoch, args.num_epochs):\n",
    "    if args.resume_training:\n",
    "        if epoch %1 == 0:\n",
    "            optimizer.param_groups[0][\"lr\"] /= 2\n",
    "            print('learning rate reduced by factor of 2')\n",
    "\n",
    "    train_bar = utils.ProgressBar(train_loader, epoch)\n",
    "    for meter in train_meters.values():\n",
    "        meter.reset()\n",
    "\n",
    "#     epoch_loss_sum = 0\n",
    "    for batch_id, (clean, mask) in enumerate(train_bar):\n",
    "        # dataloader returns [clean, mask] list\n",
    "        model.train()\n",
    "        global_step += 1\n",
    "        inputs = clean.to(device)\n",
    "        mask_inputs = mask.to(device)\n",
    "        # only use the mask part of the outputs\n",
    "        raw_outputs = model(inputs,mask_inputs)\n",
    "        outputs = (1-mask_inputs[:,:3,:,:])*raw_outputs + mask_inputs[:,:3,:,:]*inputs[:,:3,:,:]\n",
    "        \n",
    "        if args.wtd_loss:\n",
    "            loss = weighted_MSELoss(outputs,inputs[:,:3,:,:],sc)/(inputs.size(0)*(args.mask_pts_per_seq**2))\n",
    "            # Regularization?\n",
    "            \n",
    "        else:\n",
    "            # normalized by the number of masked points\n",
    "            loss = F.mse_loss(outputs, inputs[:,:3,:,:], reduction=\"sum\") / \\\n",
    "                   (inputs.size(0) * (args.mask_pts_per_seq**2))\n",
    "\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "#         epoch_loss_sum += loss * inputs.size(0)\n",
    "        train_meters[\"train_loss\"].update(loss)\n",
    "        train_bar.log(dict(**train_meters, lr=optimizer.param_groups[0][\"lr\"]), verbose=True)\n",
    "\n",
    "        if writer is not None and global_step % args.log_interval == 0:\n",
    "            writer.add_scalar(\"lr\", optimizer.param_groups[0][\"lr\"], global_step)\n",
    "            writer.add_scalar(\"loss/train\", loss.item(), global_step)\n",
    "            gradients = torch.cat([p.grad.view(-1) for p in model.parameters() if p.grad is not None], dim=0)\n",
    "            writer.add_histogram(\"gradients\", gradients, global_step)\n",
    "            sys.stdout.flush()\n",
    "#     epoch_loss = epoch_loss_sum / len(train_loader.dataset)\n",
    "    \n",
    "    if epoch % args.valid_interval == 0:\n",
    "        model.eval()\n",
    "        for meter in valid_meters.values():\n",
    "            meter.reset()\n",
    "\n",
    "        valid_bar = utils.ProgressBar(valid_loader)\n",
    "        val_loss = 0\n",
    "        for sample_id, (clean, mask) in enumerate(valid_bar):\n",
    "            with torch.no_grad():\n",
    "                inputs = clean.to(device)\n",
    "                mask_inputs = mask.to(device)\n",
    "                # only use the mask part of the outputs\n",
    "                raw_output = model(inputs,mask_inputs)\n",
    "                output = (1-mask_inputs[:,:3,:,:])*raw_output + mask_inputs[:,:3,:,:]*inputs[:,:3,:,:]\n",
    "\n",
    "                # TO DO, only run loss on masked part of output\n",
    "                \n",
    "                if args.wtd_loss:\n",
    "                    val_loss = weighted_MSELoss(output,inputs[:,:3,:,:],sc)/(inputs.size(0)*(args.mask_pts_per_seq**2))\n",
    "                else:\n",
    "                    # normalized by the number of masked points\n",
    "                    val_loss = F.mse_loss(output, inputs[:,:3,:,:], reduction=\"sum\")/(inputs.size(0)* \\\n",
    "                                                                                    (args.mask_pts_per_seq**2))\n",
    "\n",
    "                valid_meters[\"valid_loss\"].update(val_loss.item())\n",
    "\n",
    "        if writer is not None:\n",
    "            writer.add_scalar(\"loss/valid\", valid_meters['valid_loss'].avg, global_step)\n",
    "            sys.stdout.flush()\n",
    "\n",
    "        logging.info(train_bar.print(dict(**train_meters, **valid_meters, lr=optimizer.param_groups[0][\"lr\"])))\n",
    "        utils.save_checkpoint(args, global_step, model, optimizer, score=valid_meters[\"valid_loss\"].avg, mode=\"min\")\n",
    "    scheduler.step()\n",
    "\n",
    "logging.info(f\"Done training! Best Loss {utils.save_checkpoint.best_score:.3f} obtained after step {utils.save_checkpoint.best_step}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single output example\n",
    "i,m = next(iter(train_loader))\n",
    "inputs = i.to(device)\n",
    "mask = m.to(device)\n",
    "raw_output = model(inputs,mask)\n",
    "output_model = (1-mask[:,:3,:])*raw_output + mask[:,:3,:]*inputs[:,:3,:]\n",
    "raw_output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baselines\n",
    "First: Interpolate between last and next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "# model_cpu = model.to('cpu')\n",
    "def loss_comparison(loader,model,mask_pts_per_seq=args.mask_pts_per_seq,pt_count=len(valid_dataset)):\n",
    "    wtd_loss = True\n",
    "    loss_model = 0\n",
    "    loss_interp = 0\n",
    "    for batch_id, (i, m) in enumerate(loader):\n",
    "        with torch.no_grad():\n",
    "            # conv1D model\n",
    "            inputs = i.to(device)\n",
    "            mask = m.to(device)\n",
    "            raw_output = model(inputs,mask)\n",
    "            output_model = (1-mask[:,:3,:])*raw_output + mask[:,:3,:]*inputs[:,:3,:]\n",
    "            \n",
    "            # Interpolation\n",
    "            output_interp = lidar_data_processing.outer_interp_loop(i,m,mask_pts_per_seq,2)\n",
    "\n",
    "            if wtd_loss:\n",
    "                loss_model+=weighted_MSELoss(output_model,inputs[:,:3,:],sc)\n",
    "                loss_interp+=weighted_MSELoss(output_interp,i[:,:3,:],sc)\n",
    "            else:\n",
    "                # normalized by the number of masked points\n",
    "                loss_model += F.mse_loss(output_model, inputs[:,:3,:], reduction=\"sum\") \n",
    "                loss_interp += F.mse_loss(output_interp, i[:,:3,:], reduction=\"sum\") \n",
    "        print(\"Batch {} done\".format(batch_id))\n",
    "\n",
    "    # Normalize by number of batches\n",
    "    loss_model = loss_model/((mask_pts_per_seq**2)*pt_count)\n",
    "    loss_interp = loss_interp/((mask_pts_per_seq**2)*pt_count)\n",
    "    print(\"Validation Loss\\n\",\"*\"*30)\n",
    "    print(\"Model: {:2.2f}\".format(loss_model))\n",
    "    print(\"Interpolation: {:2.2f}\".format(loss_interp))\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, num_workers=8, shuffle=True)\n",
    "# loss_comparison(train_loader,model)\n",
    "loss_comparison(train_loader,model,pt_count=len(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=128, num_workers=8, shuffle=True)\n",
    "# loss_comparison(train_loader,model)\n",
    "loss_comparison(valid_loader,model,pt_count=len(valid_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs,mask_inputs = next(iter(valid_loader))\n",
    "# only use the mask part of the outputs\n",
    "raw_outputs = model(inputs.to(device),mask_inputs.to(device))\n",
    "outputs = (1-mask_inputs[:,:3,:,:])*raw_outputs.to('cpu') + mask_inputs[:,:3,:,:]*inputs[:,:3,:,:]\n",
    "\n",
    "output_interp = lidar_data_processing.outer_interp_loop(inputs,mask_inputs,args.mask_pts_per_seq,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "im_idx = 28\n",
    "\n",
    "def plot_infill(clean,output,mask,label_key = 'z'):\n",
    "    ''' Plotting function for 2D infill. Takes a single patch for clean,output, \n",
    "    and mask, and a label_key indicating which value (x,y, or z) to display.    \n",
    "    '''\n",
    "    # Which dimension to plot\n",
    "    xyz_dict = {'x':2,'y':3,'z':4}\n",
    "    z_val = xyz_dict[label_key]\n",
    "    # Set up plot\n",
    "    fig = plt.figure(figsize=[12,12])\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    # Plot unmasked points\n",
    "    surrounding_no_mask = mask[0] != 0\n",
    "    unmasked_arr = np.array(lidar_data_processing.surrounding_grid(clean,surrounding_no_mask))\n",
    "    ax.scatter(unmasked_arr[:,0],unmasked_arr[:,1],unmasked_arr[:,z_val], marker='+')\n",
    "\n",
    "    # Plot masked, filled points\n",
    "    surrounding_mask = mask[0] == 0\n",
    "    filled_arr = np.array(lidar_data_processing.surrounding_grid(output,surrounding_mask))\n",
    "    ax.scatter(filled_arr[:,0],filled_arr[:,1],filled_arr[:,z_val], color='r', marker='o')\n",
    "\n",
    "    # Plot original, masked points\n",
    "    masked_arr = np.array(lidar_data_processing.surrounding_grid(clean,surrounding_mask))\n",
    "    ax.scatter(masked_arr[:,0],masked_arr[:,1],masked_arr[:,z_val], color='g', marker='o')\n",
    "\n",
    "    # Labels and such\n",
    "    ax.set_xlabel('Grid Across-Flight',fontsize=15)\n",
    "    ax.set_ylabel('Grid Along-Flight',fontsize=15)\n",
    "    ax.set_zlabel(label_key+' value',fontsize=15)\n",
    "\n",
    "    plt.show()\n",
    "#     print(output.shape)\n",
    "#     print(clean.shape)\n",
    "#     print(weighted_MSELoss(output.unsqueeze(0),clean[:,:3,:].unsqueeze(0),sc))\n",
    "          \n",
    "plot_infill(inputs[im_idx],outputs[im_idx],mask_inputs[im_idx],'z')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_infill(inputs[im_idx],output_interp[im_idx],mask_inputs[im_idx],'z')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly\n",
    "import plotly.graph_objs as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure Plotly to be rendered inline in the notebook.\n",
    "plotly.offline.init_notebook_mode()\n",
    "\n",
    "# Configure the trace.\n",
    "trace = go.Scatter3d(\n",
    "    x=outputs.detach()[0,0,:,5],  # <-- Put your data instead\n",
    "    y=outputs.detach()[0,1,:,5],  # <-- Put your data instead\n",
    "    z=outputs.detach()[0,2,:,5],  # <-- Put your data instead\n",
    "    mode='markers',\n",
    "    marker={\n",
    "        'size': 10,\n",
    "        'opacity': 0.8,\n",
    "    }\n",
    ")\n",
    "\n",
    "# Configure the layout.\n",
    "layout = go.Layout(\n",
    "    margin={'l': 0, 'r': 0, 'b': 0, 't': 0}\n",
    ")\n",
    "\n",
    "data = [trace]\n",
    "\n",
    "plot_figure = go.Figure(data=data, layout=layout)\n",
    "\n",
    "# Render the plot.\n",
    "plotly.offline.iplot(plot_figure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
