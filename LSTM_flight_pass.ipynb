{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-6767800d6c1b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensorboard\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSummaryWriter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'utils'"
     ]
    }
   ],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "# from laspy.file import File\n",
    "# from pickle import dump, load\n",
    "\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "# import torch.optim as optim\n",
    "# import torch.utils.data as udata\n",
    "# from torch.autograd import Variable\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# %matplotlib inline\n",
    "\n",
    "import argparse\n",
    "import logging\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from utils import data\n",
    "import models, utils\n",
    "\n",
    "import pandas as pd\n",
    "from laspy.file import File\n",
    "from pickle import dump, load\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as udata\n",
    "from torch.autograd import Variable\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Data parameters\n",
    "scan_line_gap_break = 7000 # threshold over which scan_gap indicates a new scan line\n",
    "min_pt_count = 1700 # in a scan line, otherwise line not used\n",
    "max_pt_count = 2000 # in a scan line, otherwise line not used\n",
    "seq_len = 100\n",
    "num_scan_lines = 150 # to use as training set\n",
    "val_split = 0.2\n",
    "\n",
    "# LSTM Model parameters\n",
    "hidden_size = 100 # hidden features\n",
    "num_layers = 2 # Default is 1, 2 is a stacked LSTM\n",
    "output_dim = 3 # x,y,z\n",
    "\n",
    "# Training parameters\n",
    "num_epochs = 500\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpu or cpu\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "first_return_df has been processed in the following ways:  \n",
    "* Removed outliers outside of [0.01,0.99] percentile range\n",
    "* Normalized xyz values to [0,1]\n",
    "* Mapped each point to a scan line index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_return_df = pd.read_pickle(\"../../Data/parking_lot/first_returns_modified_164239.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: x_scaled, y_scaled, and z_scaled MUST be the first 3 features\n",
    "feature_list = [\n",
    "    'x_scaled',\n",
    "    'y_scaled',\n",
    "    'z_scaled',\n",
    "#     'scan_line_idx',\n",
    "#     'scan_angle_deg',\n",
    "#     'abs_scan_angle_deg'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mask the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# miss_pts_before is the count of missing points before the point in question (scan gap / 5 -1)\n",
    "first_return_df['miss_pts_before'] = round((first_return_df['scan_gap']/-5)-1)\n",
    "first_return_df['miss_pts_before'] = [max(0,pt) for pt in first_return_df['miss_pts_before']]\n",
    "\n",
    "# Add 'mask' column, set to one by default\n",
    "first_return_df['mask'] = [1]*first_return_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_missing_pts(first_return_df):\n",
    "    # Create a series with the indices of points after gaps and the number of missing points (max of 5)\n",
    "    miss_pt_ser = first_return_df[(first_return_df['miss_pts_before']>0)&\\\n",
    "                                      (first_return_df['miss_pts_before']<6)]['miss_pts_before']\n",
    "    # miss_pts_arr is an array of zeros that is the dimensions [num_missing_pts,cols_in_df]\n",
    "    miss_pts_arr = np.zeros([int(miss_pt_ser.sum()),first_return_df.shape[1]])\n",
    "    # Create empty series to collect the indices of the missing points\n",
    "    indices = np.ones(int(miss_pt_ser.sum()))\n",
    "\n",
    "    # Fill in the indices, such that they all slot in in order before the index\n",
    "    i=0\n",
    "    for index, row in zip(miss_pt_ser.index,miss_pt_ser):\n",
    "        new_indices = index + np.arange(row)/row-1+.01\n",
    "        indices[i:i+int(row)] = new_indices\n",
    "        i+=int(row)\n",
    "    # Create a Dataframe of the indices and miss_pts_arr\n",
    "    miss_pts_df = pd.DataFrame(miss_pts_arr,index=indices,columns = first_return_df.columns)\n",
    "    miss_pts_df['mask'] = [0]*miss_pts_df.shape[0]\n",
    "    # Fill scan fields with NaN so we can interpolate them\n",
    "    for col in ['scan_angle','scan_angle_deg']:\n",
    "        miss_pts_df[col] = [np.NaN]*miss_pts_df.shape[0]\n",
    "    # Concatenate first_return_df and new df\n",
    "    full_df = first_return_df.append(miss_pts_df, ignore_index=False)\n",
    "    # Resort so that the missing points are interspersed, and then reset the index\n",
    "    full_df = full_df.sort_index().reset_index(drop=True)\n",
    "    return full_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_return_df = add_missing_pts(first_return_df)\n",
    "first_return_df[['scan_angle','scan_angle_deg']] = first_return_df[['scan_angle','scan_angle_deg']].interpolate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_return_df['abs_scan_angle_deg'] = abs(first_return_df['scan_angle_deg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>X</th>\n",
       "      <th>Y</th>\n",
       "      <th>Z</th>\n",
       "      <th>intensity</th>\n",
       "      <th>flag_byte</th>\n",
       "      <th>classification_flags</th>\n",
       "      <th>classification_byte</th>\n",
       "      <th>user_data</th>\n",
       "      <th>scan_angle</th>\n",
       "      <th>...</th>\n",
       "      <th>z_scaled</th>\n",
       "      <th>adj_gps_time</th>\n",
       "      <th>num_returns</th>\n",
       "      <th>return_num</th>\n",
       "      <th>scan_gap</th>\n",
       "      <th>scan_angle_deg</th>\n",
       "      <th>scan_line_idx</th>\n",
       "      <th>miss_pts_before</th>\n",
       "      <th>mask</th>\n",
       "      <th>abs_scan_angle_deg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9780</th>\n",
       "      <td>51161.0</td>\n",
       "      <td>465708.0</td>\n",
       "      <td>3034892.0</td>\n",
       "      <td>32216.0</td>\n",
       "      <td>1753.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-3072.0</td>\n",
       "      <td>...</td>\n",
       "      <td>32.216</td>\n",
       "      <td>0.348554</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-6.0</td>\n",
       "      <td>-18.432</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>18.432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9781</th>\n",
       "      <td>51162.0</td>\n",
       "      <td>466242.0</td>\n",
       "      <td>3034757.0</td>\n",
       "      <td>32226.0</td>\n",
       "      <td>1490.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-3077.0</td>\n",
       "      <td>...</td>\n",
       "      <td>32.226</td>\n",
       "      <td>0.348557</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>-18.462</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>18.462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9782</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-3082.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-18.492</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>18.492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9783</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-3087.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-18.522</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>18.522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9784</th>\n",
       "      <td>51163.0</td>\n",
       "      <td>467975.0</td>\n",
       "      <td>3034317.0</td>\n",
       "      <td>32197.0</td>\n",
       "      <td>1357.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-3092.0</td>\n",
       "      <td>...</td>\n",
       "      <td>32.197</td>\n",
       "      <td>0.348565</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-15.0</td>\n",
       "      <td>-18.552</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>18.552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9785</th>\n",
       "      <td>51164.0</td>\n",
       "      <td>468571.0</td>\n",
       "      <td>3034166.0</td>\n",
       "      <td>32215.0</td>\n",
       "      <td>347.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-3097.0</td>\n",
       "      <td>...</td>\n",
       "      <td>32.215</td>\n",
       "      <td>0.348567</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>-18.582</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>18.582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9786</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-3102.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-18.615</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>18.615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9787</th>\n",
       "      <td>51165.0</td>\n",
       "      <td>469711.0</td>\n",
       "      <td>3033876.0</td>\n",
       "      <td>32248.0</td>\n",
       "      <td>1812.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-3108.0</td>\n",
       "      <td>...</td>\n",
       "      <td>32.248</td>\n",
       "      <td>0.348572</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-11.0</td>\n",
       "      <td>-18.648</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>18.648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9788</th>\n",
       "      <td>51166.0</td>\n",
       "      <td>470284.0</td>\n",
       "      <td>3033731.0</td>\n",
       "      <td>32278.0</td>\n",
       "      <td>1979.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-3113.0</td>\n",
       "      <td>...</td>\n",
       "      <td>32.278</td>\n",
       "      <td>0.348575</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>-18.678</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>18.678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9789</th>\n",
       "      <td>51167.0</td>\n",
       "      <td>470838.0</td>\n",
       "      <td>3033591.0</td>\n",
       "      <td>32221.0</td>\n",
       "      <td>2047.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-3118.0</td>\n",
       "      <td>...</td>\n",
       "      <td>32.221</td>\n",
       "      <td>0.348577</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>-18.708</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>18.708</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        index         X          Y        Z  intensity  flag_byte  \\\n",
       "9780  51161.0  465708.0  3034892.0  32216.0     1753.0       17.0   \n",
       "9781  51162.0  466242.0  3034757.0  32226.0     1490.0       17.0   \n",
       "9782      0.0       0.0        0.0      0.0        0.0        0.0   \n",
       "9783      0.0       0.0        0.0      0.0        0.0        0.0   \n",
       "9784  51163.0  467975.0  3034317.0  32197.0     1357.0       17.0   \n",
       "9785  51164.0  468571.0  3034166.0  32215.0      347.0       17.0   \n",
       "9786      0.0       0.0        0.0      0.0        0.0        0.0   \n",
       "9787  51165.0  469711.0  3033876.0  32248.0     1812.0       17.0   \n",
       "9788  51166.0  470284.0  3033731.0  32278.0     1979.0       17.0   \n",
       "9789  51167.0  470838.0  3033591.0  32221.0     2047.0       17.0   \n",
       "\n",
       "      classification_flags  classification_byte  user_data  scan_angle  ...  \\\n",
       "9780                   0.0                  0.0        0.0     -3072.0  ...   \n",
       "9781                   0.0                  0.0        0.0     -3077.0  ...   \n",
       "9782                   0.0                  0.0        0.0     -3082.0  ...   \n",
       "9783                   0.0                  0.0        0.0     -3087.0  ...   \n",
       "9784                   0.0                  0.0        0.0     -3092.0  ...   \n",
       "9785                   0.0                  0.0        0.0     -3097.0  ...   \n",
       "9786                   0.0                  0.0        0.0     -3102.5  ...   \n",
       "9787                   0.0                  0.0        0.0     -3108.0  ...   \n",
       "9788                   0.0                  0.0        0.0     -3113.0  ...   \n",
       "9789                   0.0                  0.0        0.0     -3118.0  ...   \n",
       "\n",
       "      z_scaled  adj_gps_time  num_returns  return_num  scan_gap  \\\n",
       "9780    32.216      0.348554          1.0         1.0      -6.0   \n",
       "9781    32.226      0.348557          1.0         1.0      -5.0   \n",
       "9782     0.000      0.000000          0.0         0.0       0.0   \n",
       "9783     0.000      0.000000          0.0         0.0       0.0   \n",
       "9784    32.197      0.348565          1.0         1.0     -15.0   \n",
       "9785    32.215      0.348567          1.0         1.0      -5.0   \n",
       "9786     0.000      0.000000          0.0         0.0       0.0   \n",
       "9787    32.248      0.348572          1.0         1.0     -11.0   \n",
       "9788    32.278      0.348575          1.0         1.0      -5.0   \n",
       "9789    32.221      0.348577          1.0         1.0      -5.0   \n",
       "\n",
       "      scan_angle_deg  scan_line_idx  miss_pts_before  mask  abs_scan_angle_deg  \n",
       "9780         -18.432            0.0              0.0     1              18.432  \n",
       "9781         -18.462            0.0              0.0     1              18.462  \n",
       "9782         -18.492            0.0              0.0     0              18.492  \n",
       "9783         -18.522            0.0              0.0     0              18.522  \n",
       "9784         -18.552            0.0              2.0     1              18.552  \n",
       "9785         -18.582            0.0              0.0     1              18.582  \n",
       "9786         -18.615            0.0              0.0     0              18.615  \n",
       "9787         -18.648            0.0              1.0     1              18.648  \n",
       "9788         -18.678            0.0              0.0     1              18.678  \n",
       "9789         -18.708            0.0              0.0     1              18.708  \n",
       "\n",
       "[10 rows x 24 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_return_df.iloc[9780:9790]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Extract tensor of scan lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of points per scan line\n",
    "scan_line_pt_count = first_return_df.groupby('scan_line_idx').count()['gps_time']\n",
    "\n",
    "# Identify the indices for points at end of scan lines\n",
    "scan_break_idx = first_return_df[(first_return_df['scan_gap']>scan_line_gap_break)].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Tensor\n",
    "line_count = ((scan_line_pt_count>min_pt_count)&(scan_line_pt_count<max_pt_count)).sum()\n",
    "scan_line_tensor = torch.randn([line_count,min_pt_count,len(feature_list)])\n",
    "\n",
    "# Collect the scan lines longer than min_pt_count\n",
    "# For each, collect the first min_pt_count points\n",
    "i=0\n",
    "for line,count in enumerate(scan_line_pt_count):\n",
    "    if (count>min_pt_count)&(count<max_pt_count):\n",
    "        try:\n",
    "            line_idx = scan_break_idx[line-1]\n",
    "            scan_line_tensor[i,:,:] = torch.Tensor(first_return_df.iloc\\\n",
    "                                      [line_idx:line_idx+min_pt_count][feature_list].values)\n",
    "            i+=1\n",
    "        except RuntimeError:\n",
    "            print(\"line: \",line)\n",
    "            print(\"line_idx: \",line_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Setting all features to [0,1] overvalues the z coordinate in MSE Loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_tensor(tensor):\n",
    "    # Function takes a 3-D tensor, performs minmax scaling to [0,1] along the third dimension.\n",
    "    # First 2 dimensions are flattened\n",
    "    a,b,c = tensor.shape\n",
    "    # Flatten first two dimensions\n",
    "    flat_tensor = tensor.view(-1,c)\n",
    "    sc =  MinMaxScaler()\n",
    "    flat_norm_tensor = sc.fit_transform(flat_tensor)\n",
    "    # Reshape to original\n",
    "    output = flat_norm_tensor.reshape([a,b,c])\n",
    "    return torch.Tensor(output), sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "scan_line_tensor_norm, sc = min_max_tensor(scan_line_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) Generate the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_windows(data, seq_length, line_num, x, y):\n",
    "    for i in range(len(data)-seq_length):\n",
    "        # Index considers previous lines\n",
    "        idx = i+line_num*(min_pt_count-seq_length)\n",
    "        _x = data[i:(i+seq_length)]\n",
    "        _y = data[i+seq_length,:3] # Assumes xyz are the first 3 features in scan_line_tensor\n",
    "        x[idx,:,:] = _x\n",
    "        y[idx,:,:] = _y\n",
    "\n",
    "    return x,y\n",
    "\n",
    "def generate_samples(data,min_pt_count,seq_len,num_scan_lines,val_split,starting_line=1000):\n",
    "    '''\n",
    "    Function generates training and validation samples for predicting the next point in the sequence.\n",
    "    Inputs:\n",
    "        data: 3-Tensor with dimensions: i) the number of viable scan lines in the flight pass, \n",
    "                                        ii) the minimum number of points in the scan line,\n",
    "                                        iii) 3 (xyz, or feature count)\n",
    "    \n",
    "    '''\n",
    "    # Create generic x and y tensors\n",
    "    x = torch.ones([(min_pt_count-seq_len)*num_scan_lines,seq_len,len(feature_list)]) \n",
    "    y = torch.ones([(min_pt_count-seq_len)*num_scan_lines,1,3])\n",
    "    i=0\n",
    "    # Cycle through the number of scan lines requested, starting somewhere in the middle\n",
    "    for line_idx in range(starting_line,starting_line+num_scan_lines):\n",
    "        x,y = sliding_windows(data[line_idx,:,:],seq_len,line_idx-starting_line, x, y)\n",
    "    x_train,y_train,x_val,y_val = train_val_split(x,y,val_split)\n",
    "    return x_train,y_train,x_val,y_val\n",
    "\n",
    "def train_val_split(x,y,val_split):   \n",
    "    # Training/Validation split\n",
    "    # For now, we'll do the last part of the dataset as validation...shouldn't matter?\n",
    "    train_val_split_idx = int(x.shape[0]*(1-val_split))\n",
    "    x_train = x[:train_val_split_idx,:,:]\n",
    "    x_val = x[train_val_split_idx:,:,:]\n",
    "    y_train = y[:train_val_split_idx,:,:]\n",
    "    y_val = y[train_val_split_idx:,:,:]\n",
    "    \n",
    "    return x_train,y_train,x_val,y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,y_train,x_val,y_val = generate_samples(scan_line_tensor_norm,min_pt_count,seq_len,num_scan_lines,val_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2: Train the model  \n",
    "Borrowing a lot of code from here: https://github.com/spdin/time-series-prediction-lstm-pytorch/blob/master/Time_Series_Prediction_with_LSTM_Using_PyTorch.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, output_dim, input_size, hidden_size, num_layers, seq_len):\n",
    "        super(LSTM, self).__init__()\n",
    "        # output_dim = 3: X,Y,Z\n",
    "        self.output_dim = output_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # inputs_size = 3: X,Y,Z (could be larger in the future if we add features here)\n",
    "        self.input_size = input_size\n",
    "        \n",
    "        # Not sure what to do here, larger than input size?\n",
    "        self.hidden_size = hidden_size\n",
    "        # Passes from above\n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size,\n",
    "                            num_layers=num_layers, batch_first=True)\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_size, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.lstm.flatten_parameters()\n",
    "        h_0 = Variable(torch.zeros(\n",
    "            self.num_layers, x.size(0), self.hidden_size)).to(device)\n",
    "        \n",
    "        c_0 = Variable(torch.zeros(\n",
    "            self.num_layers, x.size(0), self.hidden_size)).to(device)\n",
    "        \n",
    "        # Propagate input through LSTM\n",
    "        ula, (h_out, _) = self.lstm(x, (h_0, c_0))\n",
    "        \n",
    "        # In case multiple LSTM layers are used, this predicts using only the last layer\n",
    "        h_out = h_out.view(num_layers,-1, self.hidden_size)\n",
    "        out = self.fc(h_out[-1,:,:])\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a loss function that weights the loss according to coordinate ranges (xmax-xmin, ymax-ymin, zmax-zmin)\n",
    "def weighted_MSELoss(pred,true,sc):\n",
    "    '''Assumes that x,y,z are the first 3 features in sc scaler'''\n",
    "    ranges = torch.Tensor(sc.data_max_[:3]-sc.data_min_[:3])\n",
    "    raw_loss = torch.zeros(3,dtype=float)\n",
    "    crit = torch.nn.MSELoss()\n",
    "    for i in range(3):\n",
    "        raw_loss[i] = crit(pred[:,:,i], true[:,:,i])\n",
    "    return (ranges * raw_loss).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(lstm,x,y,ltype='Training'):\n",
    "    # Training loss\n",
    "    y_pred = lstm(x).detach().to('cpu')\n",
    "    loss = weighted_MSELoss(y_pred.unsqueeze(1), y,sc)\n",
    "    print(\"{} Loss: {:.4f}\".format(ltype,loss))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LidarLstmDataset(udata.Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        super(LidarLstmDataset, self).__init__()\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.x.shape[0]\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        return self.x[index],self.y[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024, 100, 6])\n",
      "torch.Size([1024, 100, 6])\n",
      "torch.Size([1024, 100, 6])\n",
      "torch.Size([1024, 100, 6])\n",
      "torch.Size([1024, 100, 6])\n",
      "torch.Size([1024, 100, 6])\n",
      "torch.Size([1024, 100, 6])\n",
      "torch.Size([1024, 100, 6])\n",
      "torch.Size([1024, 100, 6])\n",
      "torch.Size([1024, 100, 6])\n",
      "torch.Size([1024, 100, 6])\n",
      "torch.Size([1024, 100, 6])\n",
      "torch.Size([1024, 100, 6])\n",
      "torch.Size([1024, 100, 6])\n",
      "torch.Size([1024, 100, 6])\n",
      "torch.Size([1024, 100, 6])\n",
      "torch.Size([1024, 100, 6])\n",
      "torch.Size([1024, 100, 6])\n",
      "torch.Size([1024, 100, 6])\n",
      "torch.Size([1024, 100, 6])\n",
      "torch.Size([1024, 100, 6])\n",
      "torch.Size([1024, 100, 6])\n",
      "torch.Size([1024, 100, 6])\n",
      "torch.Size([1024, 100, 6])\n",
      "torch.Size([1024, 100, 6])\n",
      "torch.Size([1024, 100, 6])\n",
      "torch.Size([1024, 100, 6])\n",
      "torch.Size([1024, 100, 6])\n",
      "torch.Size([1024, 100, 6])\n",
      "torch.Size([1024, 100, 6])\n",
      "torch.Size([1024, 100, 6])\n",
      "torch.Size([1024, 100, 6])\n",
      "torch.Size([1024, 100, 6])\n",
      "torch.Size([1024, 100, 6])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-28cebdd2f7c9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m# obtain the loss function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweighted_MSELoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_loss,vl = [],[]\n",
    "lstm_local = LSTM(output_dim, len(feature_list), hidden_size, num_layers, seq_len)\n",
    "lstm = lstm_local.to(device)\n",
    "\n",
    "# Create the dataloader\n",
    "train_dataset = LidarLstmDataset(x_train,y_train)\n",
    "val_dataset = LidarLstmDataset(x_val,y_val)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=1024, num_workers=4, shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(val_dataset, batch_size=1, num_workers=4, shuffle=False)\n",
    "\n",
    "# criterion = torch.nn.MSELoss()    # mean-squared error for regression\n",
    "optimizer = torch.optim.Adam(lstm.parameters(), lr=learning_rate)\n",
    "# Scheduler to reduce the learning rate\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 50, gamma=0.5)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    for x,y in train_loader:\n",
    "        print(x.shape)\n",
    "        outputs = lstm(x.to(device))\n",
    "        optimizer.zero_grad()\n",
    "        # obtain the loss function\n",
    "        loss = weighted_MSELoss(outputs.unsqueeze(1), y.to(device),sc)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        batch_loss.append(loss)\n",
    "    print(\"Epoch: %d, Training batch loss: %1.5f\\n\" % (epoch, loss.item()))\n",
    "    scheduler.step()\n",
    "    if epoch % 5 == 0:\n",
    "        print(\"*\"*30)\n",
    "        val = calculate_loss(lstm,x_val.to(device),y_val,'Validation')\n",
    "        print(\"*\"*30)\n",
    "        vl.append(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-6b526450ab2a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_loader' is not defined"
     ]
    }
   ],
   "source": [
    "for x,y in train_loader:\n",
    "    print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print loss plot\n",
    "plt.subplot(2,1,1)\n",
    "plt.plot(20*np.arange(len(batch_loss)-10),tl[10:],label='Training')\n",
    "plt.xlabel(\"Batch\")\n",
    "plt.ylabel(\"Weighted MSE\")\n",
    "\n",
    "plt.subplot(2,1,2)\n",
    "plt.plot(20*np.arange(len(vl)-10),vl[10:],'+',label='Validation')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Weighted MSE\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the model\n",
    "dir_name = '8_31_20/'\n",
    "run_descriptor = 'seq_len_100_hidden_size_100'\n",
    "scaler = load(open(\"models/\"+dir_name+\"SCALER_\"+run_descriptor+\".pkl\",'rb'))\n",
    "lstm = load(open(\"models/\"+dir_name+run_descriptor+\".pkl\",'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results(x,y,lstm,sc,sample_num,transform=False):\n",
    "    markersize,fontsize=12,14\n",
    "    if transform:\n",
    "        in_seq = sc.inverse_transform(x[sample_num])\n",
    "        pred_norm = (lstm(x[sample_num].unsqueeze(0).to(device)).view(-1,3).detach())\n",
    "        pred_point =     pred_norm.to('cpu')*(sc.data_max_[:3]-sc.data_min_[:3])+sc.data_min_[:3]\n",
    "        true_point = y[sample_num]*(sc.data_max_[:3]-sc.data_min_[:3])+sc.data_min_[:3]\n",
    "    else:\n",
    "        in_seq = x[sample_num]\n",
    "        pred_point = (lstm(x[sample_num].unsqueeze(0).to(device)).view(-1,3).detach()).to('cpu')\n",
    "        true_point = y[sample_num]\n",
    "        \n",
    "    plt.figure(figsize=[12,12])\n",
    "    plt.subplot(2,1,1)\n",
    "    plt.plot(in_seq[:,0],in_seq[:,1],'x',label='sequence')\n",
    "    plt.plot(pred_point[0,0],pred_point[0,1],'ro',markersize=markersize,label='Prediction')\n",
    "    plt.plot(true_point[0,0],true_point[0,1],'go',markersize=markersize,label='True')\n",
    "    plt.xlabel(\"X\",fontsize=fontsize)\n",
    "    plt.ylabel(\"Y\",fontsize=fontsize)\n",
    "    plt.xticks(fontsize=fontsize)\n",
    "    plt.yticks(fontsize=fontsize)\n",
    "    plt.legend(fontsize=fontsize)\n",
    "    \n",
    "    plt.subplot(2,1,2)\n",
    "    plt.plot(in_seq[:,0],in_seq[:,2],'x',label='sequence')\n",
    "    plt.plot(pred_point[0,0],pred_point[0,2],'ro',markersize=markersize,label='Prediction')\n",
    "    plt.plot(true_point[0,0],true_point[0,2],'go',markersize=markersize,label='True')\n",
    "    plt.xlabel(\"X\",fontsize=fontsize)\n",
    "    plt.ylabel(\"Z\",fontsize=fontsize)\n",
    "    plt.xticks(fontsize=fontsize)\n",
    "    plt.yticks(fontsize=fontsize)\n",
    "    plt.legend(fontsize=fontsize)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(4120,4125):\n",
    "    print_results(x_train,y_train,lstm,scaler,i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "dir_name = '8_31_20/'\n",
    "run_descriptor = 'seq_len_100_hidden_size_100'\n",
    "os.mkdir('models/'+dir_name)\n",
    "class Args(object):\n",
    "    def __init__(self):\n",
    "        self.scan_line_gap_break = scan_line_gap_break\n",
    "        self.min_pt_count = min_pt_count\n",
    "        self.max_pt_count = max_pt_count\n",
    "        self.seq_len = seq_len\n",
    "        self.num_scan_lines = num_scan_lines\n",
    "        self.val_split = val_split\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.output_dim = output_dim\n",
    "        self.num_epochs = num_epochs\n",
    "        self.learning_rate = learning_rate\n",
    "args=Args()\n",
    "\n",
    "# Save the scaler\n",
    "dump(lstm, open('models/'+dir_name+run_descriptor+'.pkl','wb'))\n",
    "dump(sc, open('models/'+dir_name+'SCALER_'+run_descriptor+'.pkl', 'wb'))\n",
    "dump(args, open('models/'+dir_name+'args_'+run_descriptor+'.pkl', 'wb'))\n",
    "with open('models/'+dir_name+'args_'+run_descriptor+'.json', 'w') as json_file:\n",
    "    json.dump(json.dumps(args.__dict__), json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create .pts file of predictions\n",
    "Include the actual and the predicted, indicated with a binary flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pts_file(y_val,x_val,lstm,sc):\n",
    "    y_val_reinflate = np.concatenate((y_val[:,0,:]*(sc.data_max_[:3]-sc.data_min_[:3]) \\\n",
    "                                      +sc.data_min_[:3],np.zeros((y_val.shape[0],1))),axis=1)\n",
    "    out_df = pd.DataFrame(np.array(y_val_reinflate[:,:]),columns=['x','y','z','class'])\n",
    "    pred_norm = (lstm(x_val).view(-1,3).detach())\n",
    "    pred_reinflate = pred_norm*(sc.data_max_[:3]-sc.data_min_[:3])+sc.data_min_[:3]\n",
    "    pred_arr = np.concatenate((pred_reinflate,np.ones((pred_reinflate.shape[0],1))),axis=1)\n",
    "    out_df = out_df.append(pd.DataFrame(pred_arr,columns = out_df.columns)).reset_index(drop=True)\n",
    "    return out_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_df = create_pts_file(y_val,x_val,lstm,sc)\n",
    "out_df.to_csv(\"output_test.pts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Prep\n",
    "Already done, but this removes outliers and adds scan_line_idx to the first_return_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adj GPS Time: Set both timestamps to zero for the first record\n",
    "def adjust_time(df,time_field):\n",
    "    # Function adds adj_gps_time to points or pulse dataframe, set to zero at the minimum timestamp.\n",
    "    df['adj_gps_time'] = df[time_field] - df[time_field].min()\n",
    "    return df\n",
    "\n",
    "def label_returns(las_df):\n",
    "    '''\n",
    "    Parses the flag_byte into number of returns and return number, adds these fields to las_df.\n",
    "    Input - las_df - dataframe from .laz or .lz file\n",
    "    Output - first_return_df - only the first return points from las_df.\n",
    "           - las_df - input dataframe with num_returns and return_num fields added \n",
    "    '''\n",
    "    \n",
    "    las_df['num_returns'] = np.floor(las_df['flag_byte']/16).astype(int)\n",
    "    las_df['return_num'] = las_df['flag_byte']%16\n",
    "    first_return_df = las_df[las_df['return_num']==1]\n",
    "    first_return_df = first_return_df.reset_index(drop=True)\n",
    "    return first_return_df, las_df\n",
    "\n",
    "\n",
    "def pull_first_scan_gap(df):\n",
    "    # Separate return num, only keep the first returns, add scan_gap, sort\n",
    "    df['num_returns'] = np.floor(df['flag_byte']/16).astype(int)\n",
    "    df['return_num'] = df['flag_byte']%16\n",
    "    \n",
    "    first_return_wall = df[df['return_num']==1]\n",
    "    \n",
    "    # Outliers\n",
    "    # Remove outliers outside of [.01,.99] percentiles\n",
    "    a = first_return_wall[['x_scaled','y_scaled','z_scaled']].quantile([.01,.99])\n",
    "    first_return_wall = first_return_wall[(first_return_wall['x_scaled']>a.iloc[0]['x_scaled'])&\\\n",
    "                                         (first_return_wall['x_scaled']<a.iloc[1]['x_scaled'])&\\\n",
    "                                         (first_return_wall['y_scaled']>a.iloc[0]['y_scaled'])&\\\n",
    "                                         (first_return_wall['y_scaled']<a.iloc[1]['y_scaled'])&\\\n",
    "                                         (first_return_wall['z_scaled']>a.iloc[0]['z_scaled'])&\\\n",
    "                                         (first_return_wall['z_scaled']<a.iloc[1]['z_scaled'])]\n",
    "    \n",
    "    first_return_wall.sort_values(by=['gps_time'],inplace=True)\n",
    "    first_return_wall.reset_index(inplace=True)\n",
    "    first_return_wall.loc[1:,'scan_gap'] = [first_return_wall.loc[i+1,'scan_angle'] - first_return_wall.loc[i,'scan_angle'] for i in range(first_return_wall.shape[0]-1)]\n",
    "    first_return_wall.loc[0,'scan_gap'] = 0\n",
    "    first_return_wall['scan_angle_deg'] = first_return_wall['scan_angle']*.006\n",
    "    return first_return_wall\n",
    "\n",
    "# Load LAS points\n",
    "las_df = pd.read_hdf(\"../../Data/parking_lot/las_points_164239.lz\")\n",
    "# Separate out the first returns only\n",
    "las_df = adjust_time(las_df,'gps_time')\n",
    "# Sort records by timestamp\n",
    "las_df.sort_values(by=['adj_gps_time'],inplace=True)\n",
    "# TO DO: consider only last returns?\n",
    "# First returns only\n",
    "first_return_df = pull_first_scan_gap(las_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Identify the indices for points at end of scan lines\n",
    "scan_break_idx = first_return_df[(first_return_df['scan_gap']>scan_line_gap_break)].index\n",
    "\n",
    "# # Concat adds index 0 as 0th scan line\n",
    "_right = pd.DataFrame(data=range(1,len(scan_break_idx)+1),index=scan_break_idx,columns=['scan_line_idx'])\n",
    "right = pd.concat([pd.DataFrame(data=[0],index=[0],columns=['scan_line_idx']),_right])\n",
    "first_return_df = pd.merge_asof(first_return_df,right,left_index=True,right_index=True,direction='backward')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
